{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random \n",
    "import re\n",
    "import cv2\n",
    "\n",
    "from random import shuffle\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from skimage.transform import rescale\n",
    "from skimage.transform import rotate\n",
    "from skimage import exposure\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization,SpatialDropout2D,Conv2DTranspose,concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nsre = re.compile('([0-9]+)')\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(_nsre, s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_dir(lab_file,type_file,set_file):\n",
    "    data_path='../DL_course_data/'\n",
    "    set_dir=os.path.join(data_path, lab_file,type_file,set_file)\n",
    "    return set_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_list(set_dir):\n",
    "    set_list = os.listdir(set_dir)\n",
    "    set_list.sort(key=natural_sort_key)\n",
    "    return set_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir(data_list,data_dir):\n",
    "    new_data=[]\n",
    "    for x in data_list:\n",
    "        new = os.path.join(data_dir,x)\n",
    "        new_data.append(new)\n",
    "    return new_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(lab_file,type_file,train_percent):\n",
    "    img_dir1 = create_set_dir(lab_file,type_file,'Image')\n",
    "    img_dir2 = create_set_dir(lab_file,type_file,'Mask')\n",
    "\n",
    "    image_list = create_set_list(img_dir1)\n",
    "    mask_list = create_set_list(img_dir2)\n",
    "\n",
    "    index_position = list(zip(image_list,mask_list))\n",
    "    random.shuffle(index_position)\n",
    "    image_list[:],mask_list[:] = zip(*index_position)\n",
    "\n",
    "    length = len(image_list)\n",
    "    train_length = int(length*train_percent)\n",
    "\n",
    "    x_train = image_list[0:train_length]\n",
    "    y_train = mask_list[0:train_length]\n",
    "    x_test = image_list[train_length:]\n",
    "    y_test = mask_list[train_length:]\n",
    "    \n",
    "    x_train = list_dir(x_train,img_dir1)\n",
    "    y_train = list_dir(y_train,img_dir2)\n",
    "    x_test = list_dir(x_test,img_dir1)\n",
    "    y_test = list_dir(y_test,img_dir2)\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_list,img_w,img_h,img_ch,mask=False):\n",
    "    tab = np.zeros((len(data_list),img_w,img_h,img_ch),dtype='float32')\n",
    "    for i in range(len(data_list)):\n",
    "        Img = cv2.imread(data_list[i],0)\n",
    "        Img = cv2.resize(Img,(img_w, img_h))\n",
    "        Img = Img.reshape(img_w,img_h)/255\n",
    "        if mask:\n",
    "            Img[Img>0]=1\n",
    "            Img[Img!=1]=0\n",
    "        tab[i,:,:,0]=Img\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(image_set,mask_set,dictionary_augmentation,batch_size):\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**dictionary_augmentation)\n",
    "    mask_datagen = ImageDataGenerator(**dictionary_augmentation)\n",
    "\n",
    "    image_generator = image_datagen.flow(\n",
    "    image_set,\n",
    "    y=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=1)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow(\n",
    "    mask_set,\n",
    "    y=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=1)\n",
    "    train_generator=(pair for pair in zip(image_generator,mask_generator))\n",
    "    \n",
    "    \n",
    "    STEP_SIZE_TRAIN=(len(image_set)+len(mask_set))//batch_size\n",
    "    \n",
    "    return train_generator,STEP_SIZE_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(InputLayer,base_dense,BatchNorm=False):\n",
    "    if BatchNorm ==True:\n",
    "        conv1 = Conv2D(base_dense,(3,3),strides = (1,1), padding = \"same\")(InputLayer)\n",
    "        batch1 = BatchNormalization()(conv1)\n",
    "        act1 = Activation('relu')(batch1)\n",
    "        conv2 = Conv2D(base_dense,(3,3),strides = (1,1), padding = \"same\")(act1)\n",
    "        batch2 = BatchNormalization()(conv2)\n",
    "        act2 = Activation('relu')(batch2)\n",
    "\n",
    "    else:\n",
    "        conv1 = Conv2D(base_dense,(3,3),strides = (1,1), padding = \"same\", activation=\"relu\")(InputLayer)\n",
    "        conv2 = Conv2D(base_dense,(3,3),strides = (1,1), padding = \"same\", activation=\"relu\")(conv1)\n",
    "    return conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(base_dense,img_w,img_h,img_ch,dropout=False,dr=0.2):\n",
    "    input_size = (img_w, img_h,img_ch)\n",
    "    input_layer = Input(shape=input_size, name='input_layer')\n",
    "    \n",
    "    if dropout==True:\n",
    "        conv1 = conv_block(input_layer,base_dense,BatchNorm=False)\n",
    "        pool1 = MaxPooling2D((2,2))(conv1)\n",
    "        pool1 = Dropout(dr)(pool1)\n",
    "\n",
    "        conv2 = conv_block(pool1,base_dense*2,BatchNorm=False)\n",
    "        pool2 = MaxPooling2D((2,2))(conv2)\n",
    "        pool2 = Dropout(dr)(pool2)\n",
    "\n",
    "        conv3 = conv_block(pool2,base_dense*4,BatchNorm=False)\n",
    "        pool3 = MaxPooling2D((2,2))(conv3)\n",
    "        pool3 = Dropout(dr)(pool3)\n",
    "\n",
    "        conv4 = conv_block(pool3,base_dense*8,BatchNorm=False)\n",
    "        pool4 = MaxPooling2D((2,2))(conv4)\n",
    "        pool4 = Dropout(dr)(pool4)\n",
    "\n",
    "        #middle\n",
    "        convm = conv_block(pool4,base_dense*16,BatchNorm=False)\n",
    "\n",
    "        #deconvolution\n",
    "        deconv1 = Conv2DTranspose(base_dense*8,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(convm)\n",
    "        uconv1=concatenate([deconv1,conv4])\n",
    "        uconv1=Dropout(dr)(uconv1)\n",
    "        uconv1=conv_block(uconv1,base_dense*8,BatchNorm=False)\n",
    "\n",
    "        deconv2 = Conv2DTranspose(base_dense*4,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv1)\n",
    "        uconv2=concatenate([deconv2,conv3])\n",
    "        uconv2=Dropout(dr)(uconv2)\n",
    "        uconv2=conv_block(uconv2,base_dense*4,BatchNorm=False)\n",
    "\n",
    "        deconv3= Conv2DTranspose(base_dense*2,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv2)\n",
    "        uconv3=concatenate([deconv3,conv2])\n",
    "        uconv3=Dropout(dr)(uconv3)\n",
    "        uconv3=conv_block(uconv3,base_dense*2,BatchNorm=False)\n",
    "\n",
    "        deconv4 = Conv2DTranspose(base_dense,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv3)\n",
    "        uconv4=concatenate([deconv4,conv1])\n",
    "        uconv4=Dropout(dr)(uconv4)\n",
    "        uconv4=conv_block(uconv4,base_dense,BatchNorm=False)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        conv1 = conv_block(input_layer,base_dense,BatchNorm=False)\n",
    "        pool1 = MaxPooling2D((2,2))(conv1)\n",
    "\n",
    "        conv2 = conv_block(pool1,base_dense*2,BatchNorm=False)\n",
    "        pool2 = MaxPooling2D((2,2))(conv2)\n",
    "\n",
    "        conv3 = conv_block(pool2,base_dense*4,BatchNorm=False)\n",
    "        pool3 = MaxPooling2D((2,2))(conv3)\n",
    "\n",
    "        conv4 = conv_block(pool3,base_dense*8,BatchNorm=False)\n",
    "        pool4 = MaxPooling2D((2,2))(conv4)\n",
    "\n",
    "        #middle\n",
    "        convm = conv_block(pool4,base_dense*16,BatchNorm=False)\n",
    "\n",
    "        #deconvolution\n",
    "        deconv1 = Conv2DTranspose(base_dense*8,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(convm)\n",
    "        uconv1=concatenate([deconv1,conv4])\n",
    "        uconv1=conv_block(uconv1,base_dense*8,BatchNorm=False)\n",
    "\n",
    "        deconv2 = Conv2DTranspose(base_dense*4,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv1)\n",
    "        uconv2=concatenate([deconv2,conv3])\n",
    "        uconv2=conv_block(uconv2,base_dense*4,BatchNorm=False)\n",
    "\n",
    "        deconv3= Conv2DTranspose(base_dense*2,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv2)\n",
    "        uconv3=concatenate([deconv3,conv2])\n",
    "        uconv3=conv_block(uconv3,base_dense*2,BatchNorm=False)\n",
    "\n",
    "        deconv4 = Conv2DTranspose(base_dense,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv3)\n",
    "        uconv4=concatenate([deconv4,conv1])\n",
    "        uconv4=conv_block(uconv4,base_dense,BatchNorm=False)\n",
    "        \n",
    "    output_layer=Conv2D(1,(1,1),padding='same',activation='sigmoid',name='output_layer')(uconv4)\n",
    "    \n",
    "    model=Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_batch(base_dense,img_w,img_h,img_ch,dropout=False,dr=0.2):\n",
    "    input_size = (img_w, img_h,img_ch)\n",
    "    input_layer = Input(shape=input_size, name='input_layer')\n",
    "    \n",
    "    if dropout==True:\n",
    "        conv1 = conv_block(input_layer,base_dense,BatchNorm=True)\n",
    "        pool1 = MaxPooling2D((2,2))(conv1)\n",
    "        pool1 = Dropout(dr)(pool1)\n",
    "\n",
    "        conv2 = conv_block(pool1,base_dense*2,BatchNorm=True)\n",
    "        pool2 = MaxPooling2D((2,2))(conv2)\n",
    "        pool2 = Dropout(dr)(pool2)\n",
    "\n",
    "        conv3 = conv_block(pool2,base_dense*4,BatchNorm=True)\n",
    "        pool3 = MaxPooling2D((2,2))(conv3)\n",
    "        pool3 = Dropout(dr)(pool3)\n",
    "\n",
    "        conv4 = conv_block(pool3,base_dense*8,BatchNorm=True)\n",
    "        pool4 = MaxPooling2D((2,2))(conv4)\n",
    "        pool4 = Dropout(dr)(pool4)\n",
    "\n",
    "        #middle\n",
    "        convm = conv_block(pool4,base_dense*16,BatchNorm=True)\n",
    "\n",
    "        #deconvolution\n",
    "        deconv1 = Conv2DTranspose(base_dense*8,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(convm)\n",
    "        uconv1=concatenate([deconv1,conv4])\n",
    "        uconv1=Dropout(dr)(uconv1)\n",
    "        uconv1=conv_block(uconv1,base_dense*8,BatchNorm=True)\n",
    "\n",
    "        deconv2 = Conv2DTranspose(base_dense*4,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv1)\n",
    "        uconv2=concatenate([deconv2,conv3])\n",
    "        uconv2=Dropout(dr)(uconv2)\n",
    "        uconv2=conv_block(uconv2,base_dense*4,BatchNorm=True)\n",
    "\n",
    "        deconv3= Conv2DTranspose(base_dense*2,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv2)\n",
    "        uconv3=concatenate([deconv3,conv2])\n",
    "        uconv3=Dropout(dr)(uconv3)\n",
    "        uconv3=conv_block(uconv3,base_dense*2,BatchNorm=True)\n",
    "\n",
    "        deconv4 = Conv2DTranspose(base_dense,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv3)\n",
    "        uconv4=concatenate([deconv4,conv1])\n",
    "        uconv4=Dropout(dr)(uconv4)\n",
    "        uconv4=conv_block(uconv4,base_dense,BatchNorm=True)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        conv1 = conv_block(input_layer,base_dense,BatchNorm=True)\n",
    "        pool1 = MaxPooling2D((2,2))(conv1)\n",
    "\n",
    "        conv2 = conv_block(pool1,base_dense*2,BatchNorm=True)\n",
    "        pool2 = MaxPooling2D((2,2))(conv2)\n",
    "\n",
    "        conv3 = conv_block(pool2,base_dense*4,BatchNorm=True)\n",
    "        pool3 = MaxPooling2D((2,2))(conv3)\n",
    "\n",
    "        conv4 = conv_block(pool3,base_dense*8,BatchNorm=True)\n",
    "        pool4 = MaxPooling2D((2,2))(conv4)\n",
    "\n",
    "        #middle\n",
    "        convm = conv_block(pool4,base_dense*16,BatchNorm=True)\n",
    "\n",
    "        #deconvolution\n",
    "        deconv1 = Conv2DTranspose(base_dense*8,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(convm)\n",
    "        uconv1=concatenate([deconv1,conv4])\n",
    "        uconv1=conv_block(uconv1,base_dense*8,BatchNorm=True)\n",
    "\n",
    "        deconv2 = Conv2DTranspose(base_dense*4,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv1)\n",
    "        uconv2=concatenate([deconv2,conv3])\n",
    "        uconv2=conv_block(uconv2,base_dense*4,BatchNorm=True)\n",
    "\n",
    "        deconv3= Conv2DTranspose(base_dense*2,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv2)\n",
    "        uconv3=concatenate([deconv3,conv2])\n",
    "        uconv3=conv_block(uconv3,base_dense*2,BatchNorm=True)\n",
    "\n",
    "        deconv4 = Conv2DTranspose(base_dense,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv3)\n",
    "        uconv4=concatenate([deconv4,conv1])\n",
    "        uconv4=conv_block(uconv4,base_dense,BatchNorm=True)\n",
    "        \n",
    "    output_layer=Conv2D(1,(1,1),padding='same',activation='sigmoid',name='output_layer')(uconv4)\n",
    "    \n",
    "    model=Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit_generator(model,train_generator,x_test,y_test,loss_function, optimizer,metrics,batch_size,n_epochs,STEP_SIZE_TRAIN):\n",
    "    \n",
    "    model.compile(loss=loss_function,optimizer = optimizer,metrics=[metrics])\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    model_hist=model.fit_generator(generator=train_generator,\n",
    "                        steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                        validation_data=(x_test,y_test)\n",
    "                        ,epochs=n_epochs,verbose=1)\n",
    "    return model_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit(model,loss_function, optimizer,metrics,x_train,y_train,x_test,y_test,batch_size,n_epochs):\n",
    "    clf=model\n",
    "    clf.compile(loss=loss_function,optimizer = optimizer,metrics=[metrics])\n",
    "    clf_hist=clf.fit(x_train,y_train,batch_size,n_epochs,validation_data=(x_test, y_test))\n",
    "    return clf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_curves_plot(model_hist):\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(model_hist.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(model_hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot( np.argmin(model_hist.history[\"val_loss\"]),\n",
    "     np.min(model_hist.history[\"val_loss\"]),\n",
    "     marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss Value\")\n",
    "    plt.legend();\n",
    "    \n",
    "def accuracy_curves_plot(model_hist,metrics):\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(model_hist.history[metrics], label=\"accuracy\")\n",
    "    plt.plot(model_hist.history[\"val_\"+metrics], label=\"val_accuracy\")\n",
    "    plt.plot( np.argmax(model_hist.history[\"val_\"+metrics]),\n",
    "     np.max(model_hist.history[\"val_\"+metrics]),\n",
    "     marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy Value\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + K.epsilon()) / (K.sum(y_pred_f) + K.epsilon())\n",
    "\n",
    "def recall(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + K.epsilon()) / (K.sum(y_true_f) + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data2(lab_file,type_file):\n",
    "    img_dir1 = create_set_dir(lab_file,type_file,'Image')\n",
    "    img_dir2 = create_set_dir(lab_file,type_file,'Mask')\n",
    "    \n",
    "    image_list = create_set_list(img_dir1)\n",
    "    mask_list = create_set_list(img_dir2)\n",
    "\n",
    "    index_position = list(zip(image_list,mask_list))\n",
    "    random.shuffle(index_position)\n",
    "    image_list[:],mask_list[:] = zip(*index_position)\n",
    "    \n",
    "    \n",
    "    image_list=list_dir(image_list,img_dir1)\n",
    "    mask_list=list_dir(mask_list,img_dir2)\n",
    "\n",
    "    return image_list, mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_fold(image_list,mask_list,K_num,fold_num,img_ch):\n",
    "        \n",
    "        \n",
    "        lg=len(image_list)\n",
    "        split=int(lg/K_num)\n",
    "        \n",
    "        x_train = image_list.copy()\n",
    "        y_train = mask_list.copy()\n",
    "        x_test = image_list.copy()\n",
    "        y_test = mask_list.copy()\n",
    "        \n",
    "        x_test = image_list[fold_num*split:(fold_num+1)*split]\n",
    "        y_test = mask_list[fold_num*split:(fold_num+1)*split]\n",
    "        del x_train[fold_num*split:(fold_num+1)*split]\n",
    "        del y_train[fold_num*split:(fold_num+1)*split]\n",
    "        \n",
    "        x_train = load_data(x_train,img_w,img_h,img_ch)\n",
    "        y_train = load_data(y_train,img_w,img_h,img_ch,mask=True)\n",
    "        x_test = load_data(x_test,img_w,img_h,img_ch)\n",
    "        y_test = load_data(y_test,img_w,img_h,img_ch,mask=True)\n",
    "\n",
    "        return x_train,y_train,x_test,y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(original, filtered, filter_name):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4), sharex=True,\n",
    "                                   sharey=True)\n",
    "    ax1.imshow(original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(filtered, cmap=plt.cm.gray)\n",
    "    ax2.set_title(filter_name)\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import erosion, dilation,disk\n",
    "\n",
    "\n",
    "def create_weight_maps(mask_set,radius):\n",
    "    \n",
    "    weight_map=np.zeros((mask_set.shape[0],img_w,img_h,img_ch),dtype='float32')\n",
    "    selem=disk(radius)\n",
    "    for i in range(mask_set.shape[0]):\n",
    "        mask=np.squeeze(mask_set[i])\n",
    "        new_img=np.zeros((mask.shape[0],mask.shape[1]))\n",
    "        dilated=dilation(mask,selem)\n",
    "        eroded = erosion(mask, selem)\n",
    "        new_img=dilated-eroded\n",
    "        new_img=np.expand_dims(new_img,axis=2)\n",
    "        weight_map[i]=new_img\n",
    "    return weight_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_fold2(image_list,mask_list,K_num,fold_num):\n",
    "        \n",
    "        \n",
    "        lg=len(image_list)\n",
    "        split=int(lg/K_num)\n",
    "        \n",
    "        x_train = image_list.copy()\n",
    "        y_train = mask_list.copy()\n",
    "        x_test = image_list.copy()\n",
    "        y_test = mask_list.copy()\n",
    "        \n",
    "        x_test = image_list[fold_num*split:(fold_num+1)*split]\n",
    "        y_test = mask_list[fold_num*split:(fold_num+1)*split]\n",
    "        del x_train[fold_num*split:(fold_num+1)*split]\n",
    "        del y_train[fold_num*split:(fold_num+1)*split]\n",
    "        \n",
    "        x_train = load_data(x_train,img_w,img_h,img_ch)\n",
    "        y_train = load_data(y_train,img_w,img_h,img_ch,mask=True)\n",
    "        x_test = load_data(x_test,img_w,img_h,img_ch)\n",
    "        y_test = load_data(y_test,img_w,img_h,img_ch,mask=True)\n",
    "        \n",
    "        weight_train=create_weight_maps(y_train,2)\n",
    "        weight_test=create_weight_maps(y_test,2)\n",
    "        \n",
    "        return x_train,y_train,x_test,y_test,weight_train,weight_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(weight_map, weight_strength):\n",
    "    \n",
    "    def weighted_dice_loss(y_true, y_pred):\n",
    "        \n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        weight_f = K.flatten(weight_map)\n",
    "        weight_f = weight_f * weight_strength\n",
    "        weight_f = weight_f+1\n",
    "        weighted_intersection = K.sum(weight_f * (y_true_f * y_pred_f))\n",
    "        return -(2. * weighted_intersection + K.epsilon()) / (K.sum(y_true_f)\n",
    "        + K.sum(y_pred_f) + K.epsilon())\n",
    "    return weighted_dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_batch_weighted(base_dense,img_w,img_h,img_ch,dropout=False,dr=0.2):\n",
    "    input_size = (img_w, img_h,img_ch)\n",
    "    input_layer = Input(shape=input_size, name='input_layer')\n",
    "    loss_weights= Input((img_w, img_h,img_ch))\n",
    "    \n",
    "    if dropout==True:\n",
    "        conv1 = conv_block(input_layer,base_dense,BatchNorm=True)\n",
    "        pool1 = MaxPooling2D((2,2))(conv1)\n",
    "        pool1 = Dropout(dr)(pool1)\n",
    "\n",
    "        conv2 = conv_block(pool1,base_dense*2,BatchNorm=True)\n",
    "        pool2 = MaxPooling2D((2,2))(conv2)\n",
    "        pool2 = Dropout(dr)(pool2)\n",
    "\n",
    "        conv3 = conv_block(pool2,base_dense*4,BatchNorm=True)\n",
    "        pool3 = MaxPooling2D((2,2))(conv3)\n",
    "        pool3 = Dropout(dr)(pool3)\n",
    "\n",
    "        conv4 = conv_block(pool3,base_dense*8,BatchNorm=True)\n",
    "        pool4 = MaxPooling2D((2,2))(conv4)\n",
    "        pool4 = Dropout(dr)(pool4)\n",
    "\n",
    "        #middle\n",
    "        convm = conv_block(pool4,base_dense*16,BatchNorm=True)\n",
    "\n",
    "        #deconvolution\n",
    "        deconv1 = Conv2DTranspose(base_dense*8,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(convm)\n",
    "        uconv1=concatenate([deconv1,conv4])\n",
    "        uconv1=Dropout(dr)(uconv1)\n",
    "        uconv1=conv_block(uconv1,base_dense*8,BatchNorm=True)\n",
    "\n",
    "        deconv2 = Conv2DTranspose(base_dense*4,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv1)\n",
    "        uconv2=concatenate([deconv2,conv3])\n",
    "        uconv2=Dropout(dr)(uconv2)\n",
    "        uconv2=conv_block(uconv2,base_dense*4,BatchNorm=True)\n",
    "\n",
    "        deconv3= Conv2DTranspose(base_dense*2,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv2)\n",
    "        uconv3=concatenate([deconv3,conv2])\n",
    "        uconv3=Dropout(dr)(uconv3)\n",
    "        uconv3=conv_block(uconv3,base_dense*2,BatchNorm=True)\n",
    "\n",
    "        deconv4 = Conv2DTranspose(base_dense,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv3)\n",
    "        uconv4=concatenate([deconv4,conv1])\n",
    "        uconv4=Dropout(dr)(uconv4)\n",
    "        uconv4=conv_block(uconv4,base_dense,BatchNorm=True)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        conv1 = conv_block(input_layer,base_dense,BatchNorm=True)\n",
    "        pool1 = MaxPooling2D((2,2))(conv1)\n",
    "\n",
    "        conv2 = conv_block(pool1,base_dense*2,BatchNorm=True)\n",
    "        pool2 = MaxPooling2D((2,2))(conv2)\n",
    "\n",
    "        conv3 = conv_block(pool2,base_dense*4,BatchNorm=True)\n",
    "        pool3 = MaxPooling2D((2,2))(conv3)\n",
    "\n",
    "        conv4 = conv_block(pool3,base_dense*8,BatchNorm=True)\n",
    "        pool4 = MaxPooling2D((2,2))(conv4)\n",
    "\n",
    "        #middle\n",
    "        convm = conv_block(pool4,base_dense*16,BatchNorm=True)\n",
    "\n",
    "        #deconvolution\n",
    "        deconv1 = Conv2DTranspose(base_dense*8,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(convm)\n",
    "        uconv1=concatenate([deconv1,conv4])\n",
    "        uconv1=conv_block(uconv1,base_dense*8,BatchNorm=True)\n",
    "\n",
    "        deconv2 = Conv2DTranspose(base_dense*4,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv1)\n",
    "        uconv2=concatenate([deconv2,conv3])\n",
    "        uconv2=conv_block(uconv2,base_dense*4,BatchNorm=True)\n",
    "\n",
    "        deconv3= Conv2DTranspose(base_dense*2,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv2)\n",
    "        uconv3=concatenate([deconv3,conv2])\n",
    "        uconv3=conv_block(uconv3,base_dense*2,BatchNorm=True)\n",
    "\n",
    "        deconv4 = Conv2DTranspose(base_dense,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\")(uconv3)\n",
    "        uconv4=concatenate([deconv4,conv1])\n",
    "        uconv4=conv_block(uconv4,base_dense,BatchNorm=True)\n",
    "        \n",
    "    output_layer=Conv2D(1,(1,1),padding='same',activation='sigmoid',name='output_layer')(uconv4)\n",
    "    \n",
    "    model=Model(inputs=[input_layer,loss_weights], outputs=output_layer)\n",
    "    model.summary()\n",
    "    \n",
    "    return model,loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autocontext_fold(y_pred,f,number_of_folds,images_per_fold):\n",
    "    autocontext_val=y_pred[(f*images_per_fold):((f+1) *images_per_fold),:,:,1:]\n",
    "    size_train = y_pred.shape[0]-autocontext_val.shape[0]\n",
    "    autocontext_train= np.full((y_pred.shape[0],y_pred.shape[1],y_pred.shape[2],1),0,dtype='float32')\n",
    "    if f !=0:\n",
    "        autocontext_train[0:(f*images_per_fold)+1,:,:,1:]=y_pred[0:(f*images_per_fold)+1,:,:,1:]\n",
    "    if f!= (number_of_folds-1):\n",
    "        autocontext_train[((f+1)*images_per_fold):,:,:,1:]=y_pred[((f+1)*images_per_fold):,:,:,1:]\n",
    "    autocontext_train = autocontext_train[0:size_train]\n",
    "    return autocontext_train,autocontext_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autocontext_fold2(y_pred,f,number_of_folds,images_per_fold):\n",
    "    autocontext_val=y_pred[(f*images_per_fold):((f+1) *images_per_fold)]\n",
    "    autocontext_train= []\n",
    "    if f !=0:\n",
    "        autocontext_train[0:(f*images_per_fold)+1]=y_pred[0:(f*images_per_fold)+1]\n",
    "    if f!= (number_of_folds-1):\n",
    "        autocontext_train[((f+1)*images_per_fold):]=y_pred[((f+1)*images_per_fold):]\n",
    "    return autocontext_train,autocontext_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
